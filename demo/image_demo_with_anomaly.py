# Copyright (c) OpenMMLab. All rights reserved.
# Copyright (c) OpenMMLab. All rights reserved.
# Copyright (c) OpenMMLab. All rights reserved.
"""Image Demo with Anomaly Detection.

å¢åŠ äº†å¼‚å¸¸æ£€æµ‹åŠŸèƒ½ï¼Œå¯ä»¥è¾“å‡ºå¯èƒ½å­˜åœ¨æ¼æ£€æˆ–æ£€æµ‹é”™è¯¯çš„å›¾ç‰‡åå­—
"""

import ast
import os
import json
from argparse import ArgumentParser
from pathlib import Path

from mmengine.logging import print_log

from mmdet.apis import DetInferencer
from mmdet.evaluation import get_classes


class AnomalyDetector:
    """å¼‚å¸¸æ£€æµ‹å™¨ç±»"""

    def __init__(self,
                 enable=True,
                 min_detections=0,
                 max_detections=999,
                 min_avg_score=0.0,
                 output_file='anomaly_images.txt'):
        """
        Args:
            enable: æ˜¯å¦å¯ç”¨å¼‚å¸¸æ£€æµ‹
            min_detections: æœ€å°‘æ£€æµ‹æ•°é‡é˜ˆå€¼ï¼ˆä½äºæ­¤å€¼è§†ä¸ºæ¼æ£€ï¼‰
            max_detections: æœ€å¤šæ£€æµ‹æ•°é‡é˜ˆå€¼ï¼ˆé«˜äºæ­¤å€¼è§†ä¸ºè¿‡åº¦æ£€æµ‹ï¼‰
            min_avg_score: æœ€ä½å¹³å‡ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆä½äºæ­¤å€¼è§†ä¸ºæ£€æµ‹ä¸å¯é ï¼‰
            output_file: å¼‚å¸¸å›¾ç‰‡åˆ—è¡¨è¾“å‡ºæ–‡ä»¶
        """
        self.enable = enable
        self.min_detections = min_detections
        self.max_detections = max_detections
        self.min_avg_score = min_avg_score
        self.output_file = output_file
        self.anomaly_images = []

    def check_result(self, image_path, predictions):
        """
        æ£€æŸ¥å•å¼ å›¾ç‰‡çš„æ£€æµ‹ç»“æœæ˜¯å¦å¼‚å¸¸

        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            predictions: æ£€æµ‹ç»“æœï¼ˆå¯ä»¥æ˜¯å­—å…¸æˆ–åˆ—è¡¨ï¼‰

        Returns:
            bool: æ˜¯å¦å¼‚å¸¸
            str: å¼‚å¸¸åŸå› 
        """
        if not self.enable:
            return False, ""

        image_name = os.path.basename(image_path)

        # è§£ææ£€æµ‹ç»“æœ - é€‚é…å¤šç§è¿”å›æ ¼å¼
        pred_list = []
        scores = []

        # æ ¼å¼1: {'predictions': [...]}
        if isinstance(predictions, dict) and 'predictions' in predictions:
            pred_list = predictions['predictions']
            if pred_list and isinstance(pred_list[0], dict):
                scores = [p.get('score', p.get('scores', 0)) for p in pred_list]

        # æ ¼å¼2: {'bboxes': [...], 'scores': [...]}
        elif isinstance(predictions, dict):
            if 'bboxes' in predictions:
                pred_list = predictions['bboxes']
            if 'scores' in predictions:
                scores = predictions['scores']
            elif 'score' in predictions:
                scores = predictions['score']

        # æ ¼å¼3: ç›´æ¥æ˜¯åˆ—è¡¨
        elif isinstance(predictions, list):
            pred_list = predictions
            if pred_list and isinstance(pred_list[0], dict):
                scores = [p.get('score', p.get('scores', 0)) for p in pred_list]

        # ç¡®ä¿ scores æ˜¯åˆ—è¡¨
        if not isinstance(scores, list):
            if hasattr(scores, 'tolist'):  # numpy array
                scores = scores.tolist()
            else:
                scores = []

        num_detections = len(pred_list) if pred_list else 0

        # è°ƒè¯•ä¿¡æ¯
        print(f"  æ£€æµ‹åˆ° {num_detections} ä¸ªç›®æ ‡", end="")
        if scores:
            avg_score = sum(scores) / len(scores) if scores else 0
            print(f", å¹³å‡åˆ†: {avg_score:.3f}")
        else:
            print()

        # è§„åˆ™1: æ£€æŸ¥æ˜¯å¦æ— æ£€æµ‹ç»“æœï¼ˆå¯èƒ½æ¼æ£€ï¼‰
        if num_detections == 0:
            return True, f"æ— æ£€æµ‹ç»“æœï¼ˆå®Œå…¨æ¼æ£€ï¼‰"

        # è§„åˆ™2: æ£€æŸ¥æ£€æµ‹æ•°é‡æ˜¯å¦è¿‡å°‘ï¼ˆå¯èƒ½æ¼æ£€ï¼‰
        if num_detections < self.min_detections:
            return True, f"æ£€æµ‹æ•°é‡è¿‡å°‘ ({num_detections} < {self.min_detections})"

        # è§„åˆ™3: æ£€æŸ¥æ£€æµ‹æ•°é‡æ˜¯å¦è¿‡å¤šï¼ˆå¯èƒ½è¯¯æ£€ï¼‰
        if num_detections > self.max_detections:
            return True, f"æ£€æµ‹æ•°é‡è¿‡å¤š ({num_detections} > {self.max_detections})"

        # è§„åˆ™4: æ£€æŸ¥å¹³å‡ç½®ä¿¡åº¦æ˜¯å¦è¿‡ä½ï¼ˆå¯èƒ½æ£€æµ‹ä¸å¯é ï¼‰
        if num_detections > 0 and scores:
            avg_score = sum(scores) / len(scores)
            if avg_score < self.min_avg_score:
                return True, f"å¹³å‡ç½®ä¿¡åº¦è¿‡ä½ ({avg_score:.3f} < {self.min_avg_score})"

        return False, ""

    def add_anomaly(self, image_path, reason):
        """æ·»åŠ å¼‚å¸¸å›¾ç‰‡è®°å½•"""
        image_name = os.path.basename(image_path)
        self.anomaly_images.append({
            'image': image_name,
            'path': image_path,
            'reason': reason
        })
        print_log(f"âš ï¸  å¼‚å¸¸å›¾ç‰‡: {image_name} - {reason}", logger='current')

    def save_anomaly_list(self, output_dir):
        """ä¿å­˜å¼‚å¸¸å›¾ç‰‡åˆ—è¡¨"""
        if not self.anomaly_images:
            print_log("âœ… æœªå‘ç°å¼‚å¸¸å›¾ç‰‡", logger='current')
            return

        output_path = os.path.join(output_dir, self.output_file)

        # ä¿å­˜ä¸ºæ–‡æœ¬æ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(f"å¼‚å¸¸å›¾ç‰‡åˆ—è¡¨ (å…± {len(self.anomaly_images)} å¼ )\n")
            f.write("=" * 80 + "\n\n")
            for item in self.anomaly_images:
                f.write(f"å›¾ç‰‡: {item['image']}\n")
                f.write(f"è·¯å¾„: {item['path']}\n")
                f.write(f"åŸå› : {item['reason']}\n")
                f.write("-" * 80 + "\n")

        # åŒæ—¶ä¿å­˜ä¸ºJSONæ ¼å¼
        json_path = os.path.join(output_dir, self.output_file.replace('.txt', '.json'))
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.anomaly_images, f, ensure_ascii=False, indent=2)

        print_log(f"\nâš ï¸  å‘ç° {len(self.anomaly_images)} å¼ å¼‚å¸¸å›¾ç‰‡", logger='current')
        print_log(f"å¼‚å¸¸åˆ—è¡¨å·²ä¿å­˜åˆ°: {output_path}", logger='current')
        print_log(f"JSONæ ¼å¼å·²ä¿å­˜åˆ°: {json_path}", logger='current')

        # æ‰“å°å¼‚å¸¸ç»Ÿè®¡
        reasons = {}
        for item in self.anomaly_images:
            reason = item['reason'].split('(')[0].strip()
            reasons[reason] = reasons.get(reason, 0) + 1

        print_log("\nå¼‚å¸¸ç±»å‹ç»Ÿè®¡:", logger='current')
        for reason, count in reasons.items():
            print_log(f"  - {reason}: {count} å¼ ", logger='current')


def batch_detection_with_config():
    """
    æ‰¹é‡æ£€æµ‹å‡½æ•°ï¼Œæ‰€æœ‰å‚æ•°åœ¨ä»£ç ä¸­é…ç½®
    """

    # ================================
    # é…ç½®å‚æ•° - åœ¨è¿™é‡Œä¿®æ”¹ä½ çš„è®¾ç½®
    # ================================

    # è¾“å…¥è·¯å¾„ - å¯ä»¥æ˜¯å•å¼ å›¾ç‰‡æˆ–æ–‡ä»¶å¤¹
    inputs = '/data/home/qr/mmdetection-main/data/NWPU/val/'  # æˆ–è€… 'path/to/single_image.jpg'

    # æ¨¡å‹é…ç½®
    model = 'configs/dino/sa_dino_sr_swin_NWPU.py'
    weights = 'checkpoints/ours.pth'

    # è¾“å‡ºè®¾ç½®
    out_dir = './output_images/'  # è¾“å‡ºæ–‡ä»¶å¤¹

    # æ¨ç†è®¾ç½®
    device = 'cuda:0'
    pred_score_thr = 0.3
    batch_size = 1

    # æ˜¾ç¤ºå’Œä¿å­˜è®¾ç½®
    show = False
    no_save_vis = False
    no_save_pred = False
    print_result = True

    # å¯è§†åŒ–è®¾ç½®
    palette = 'coco'

    # æ–‡æœ¬æç¤ºè®¾ç½®
    texts = None
    custom_entities = False
    chunked_size = -1
    tokens_positive = None

    # ================================
    # ğŸ†• å¼‚å¸¸æ£€æµ‹é…ç½®
    # ================================
    enable_anomaly_check = True  # æ˜¯å¦å¯ç”¨å¼‚å¸¸æ£€æµ‹

    # å¼‚å¸¸åˆ¤æ–­è§„åˆ™ï¼ˆæ ¹æ®ä½ çš„éœ€æ±‚è°ƒæ•´ï¼‰:
    min_detections = 1      # æœ€å°‘æ£€æµ‹æ•°é‡ï¼ˆ0è¡¨ç¤ºå¿…é¡»æœ‰æ£€æµ‹ç»“æœï¼‰
    max_detections = 100    # æœ€å¤šæ£€æµ‹æ•°é‡ï¼ˆé˜²æ­¢è¿‡åº¦æ£€æµ‹ï¼‰
    min_avg_score = 0.4     # æœ€ä½å¹³å‡ç½®ä¿¡åº¦ï¼ˆä½äºæ­¤å€¼å¯èƒ½ä¸å¯é ï¼‰

    anomaly_output_file = 'anomaly_images.txt'  # å¼‚å¸¸å›¾ç‰‡åˆ—è¡¨æ–‡ä»¶å

    # ================================
    # å¼€å§‹å¤„ç†
    # ================================

    print("=" * 60)
    print("MMDetection æ‰¹é‡æ£€æµ‹å¼€å§‹")
    print("=" * 60)
    print(f"è¾“å…¥è·¯å¾„: {inputs}")
    print(f"æ¨¡å‹é…ç½®: {model}")
    print(f"æƒé‡æ–‡ä»¶: {weights}")
    print(f"è¾“å‡ºç›®å½•: {out_dir}")
    print(f"è®¾å¤‡: {device}")
    print(f"ç½®ä¿¡åº¦é˜ˆå€¼: {pred_score_thr}")

    if enable_anomaly_check:
        print("\nğŸ” å¼‚å¸¸æ£€æµ‹: å·²å¯ç”¨")
        print(f"  - æœ€å°‘æ£€æµ‹æ•°: {min_detections}")
        print(f"  - æœ€å¤šæ£€æµ‹æ•°: {max_detections}")
        print(f"  - æœ€ä½å¹³å‡åˆ†: {min_avg_score}")

    print("=" * 60)

    # æ£€æŸ¥è¾“å…¥è·¯å¾„
    if not os.path.exists(inputs):
        print(f"é”™è¯¯: è¾“å…¥è·¯å¾„ {inputs} ä¸å­˜åœ¨!")
        return

    # å¤„ç†æƒé‡æ–‡ä»¶è·¯å¾„
    if model and model.endswith('.pth'):
        print_log('æ£€æµ‹åˆ°æƒé‡æ–‡ä»¶ï¼Œè‡ªåŠ¨åˆ†é…åˆ° weights å‚æ•°')
        weights = model
        model = None

    # å¤„ç†æ–‡æœ¬æç¤º
    if texts is not None:
        if texts.startswith('$:'):
            dataset_name = texts[3:].strip()
            class_names = get_classes(dataset_name)
            texts = [tuple(class_names)]

    # å¤„ç†tokens_positive
    if tokens_positive is not None:
        tokens_positive = ast.literal_eval(tokens_positive)

    # å¤„ç†è¾“å‡ºç›®å½•
    if no_save_vis and no_save_pred:
        out_dir = ''
    elif out_dir:
        os.makedirs(out_dir, exist_ok=True)
        print(f"è¾“å‡ºç›®å½•å·²åˆ›å»º: {out_dir}")

    # åˆå§‹åŒ–å¼‚å¸¸æ£€æµ‹å™¨
    anomaly_detector = AnomalyDetector(
        enable=enable_anomaly_check,
        min_detections=min_detections,
        max_detections=max_detections,
        min_avg_score=min_avg_score,
        output_file=anomaly_output_file
    )

    # åˆå§‹åŒ–æ¨ç†å™¨
    print("æ­£åœ¨åˆå§‹åŒ–æ¨ç†å™¨...")
    try:
        inferencer = DetInferencer(
            model=model,
            weights=weights,
            device=device,
            palette=palette
        )
        print("æ¨ç†å™¨åˆå§‹åŒ–æˆåŠŸ!")
    except Exception as e:
        print(f"æ¨ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return

    # è®¾ç½®åˆ†å—å¤§å°
    if hasattr(inferencer.model, 'test_cfg'):
        inferencer.model.test_cfg.chunked_size = chunked_size

    # è·å–è¾“å…¥å›¾ç‰‡åˆ—è¡¨
    if os.path.isdir(inputs):
        image_files = []
        for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
            image_files.extend(Path(inputs).glob(f'*{ext}'))
            image_files.extend(Path(inputs).glob(f'*{ext.upper()}'))
        image_files = sorted([str(f) for f in image_files])
    else:
        image_files = [inputs]

    print(f"\næ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡")
    print("å¼€å§‹æ‰¹é‡æ¨ç†...\n")

    # é€å¼ å¤„ç†ï¼ˆç”¨äºå¼‚å¸¸æ£€æµ‹ï¼‰
    if enable_anomaly_check:
        # åˆ›å»ºä¸´æ—¶ç›®å½•ä¿å­˜å•å¼ å›¾ç‰‡çš„ç»“æœ
        temp_pred_dir = os.path.join(out_dir, 'temp_predictions') if out_dir else './temp_predictions'
        os.makedirs(temp_pred_dir, exist_ok=True)

        for idx, img_path in enumerate(image_files, 1):
            img_name = os.path.basename(img_path)
            print(f"å¤„ç† [{idx}/{len(image_files)}]: {img_name}", end=" ")

            try:
                # æ‰§è¡Œæ¨ç†ï¼Œä¿å­˜åˆ°ä¸´æ—¶ç›®å½•
                inferencer(
                    inputs=img_path,
                    out_dir=temp_pred_dir,
                    texts=texts,
                    pred_score_thr=pred_score_thr,
                    batch_size=1,
                    show=False,
                    no_save_vis=True,  # ä¸ä¿å­˜å¯è§†åŒ–ï¼ˆæé«˜é€Ÿåº¦ï¼‰
                    no_save_pred=False,  # å¿…é¡»ä¿å­˜é¢„æµ‹ç»“æœ
                    print_result=False,
                    custom_entities=custom_entities
                )

                # è¯»å–åˆšç”Ÿæˆçš„é¢„æµ‹JSONæ–‡ä»¶
                pred_json_path = os.path.join(temp_pred_dir, 'predictions.json')
                if os.path.exists(pred_json_path):
                    with open(pred_json_path, 'r') as f:
                        pred_data = json.load(f)

                    # æ‰¾åˆ°å½“å‰å›¾ç‰‡çš„é¢„æµ‹ç»“æœ
                    img_prediction = None
                    if isinstance(pred_data, list):
                        # åˆ—è¡¨æ ¼å¼ï¼š[{img1}, {img2}, ...]
                        for item in pred_data:
                            if img_name in item.get('img_path', '') or img_name == os.path.basename(item.get('img_path', '')):
                                img_prediction = item
                                break
                        # å¦‚æœæ˜¯å•å¼ å›¾ç‰‡ï¼Œå–æœ€åä¸€ä¸ª
                        if not img_prediction and len(pred_data) > 0:
                            img_prediction = pred_data[-1]
                    elif isinstance(pred_data, dict):
                        img_prediction = pred_data

                    if img_prediction:
                        # æ£€æŸ¥ç»“æœ
                        is_anomaly, reason = anomaly_detector.check_result(img_path, img_prediction)

                        if is_anomaly:
                            anomaly_detector.add_anomaly(img_path, reason)
                        else:
                            print("  âœ“")

                    # åˆ é™¤ä¸´æ—¶JSONæ–‡ä»¶
                    os.remove(pred_json_path)
                else:
                    print("  âš ï¸  æœªç”Ÿæˆé¢„æµ‹æ–‡ä»¶")

            except Exception as e:
                print(f"  âŒ å¤„ç†å¤±è´¥: {e}")
                anomaly_detector.add_anomaly(img_path, f"å¤„ç†å¼‚å¸¸: {str(e)}")

        # åˆ é™¤ä¸´æ—¶ç›®å½•
        try:
            os.rmdir(temp_pred_dir)
        except:
            pass

        # æœ€åå†ç»Ÿä¸€è¿›è¡Œä¸€æ¬¡å®Œæ•´æ¨ç†ï¼ˆå¦‚æœéœ€è¦ä¿å­˜å¯è§†åŒ–ç»“æœï¼‰
        if not no_save_vis:
            print("\næ­£åœ¨ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
            inferencer(
                inputs=inputs,
                out_dir=out_dir,
                texts=texts,
                pred_score_thr=pred_score_thr,
                batch_size=batch_size,
                show=show,
                no_save_vis=False,
                no_save_pred=not no_save_pred,
                print_result=print_result,
                custom_entities=custom_entities
            )

        # ä¿å­˜å¼‚å¸¸åˆ—è¡¨
        if out_dir:
            anomaly_detector.save_anomaly_list(out_dir)

    else:
        # ä¸å¯ç”¨å¼‚å¸¸æ£€æµ‹æ—¶ï¼Œä½¿ç”¨åŸå§‹æ‰¹é‡æ¨ç†
        try:
            inferencer(
                inputs=inputs,
                out_dir=out_dir,
                texts=texts,
                pred_score_thr=pred_score_thr,
                batch_size=batch_size,
                show=show,
                no_save_vis=no_save_vis,
                no_save_pred=no_save_pred,
                print_result=print_result,
                custom_entities=custom_entities
            )
        except Exception as e:
            print(f"æ¨ç†è¿‡ç¨‹å‡ºé”™: {e}")
            return

    if out_dir != '' and not (no_save_vis and no_save_pred):
        print_log(f'\nç»“æœå·²ä¿å­˜åˆ°: {out_dir}')

    print("\n" + "=" * 60)
    print("æ‰¹é‡æ£€æµ‹å®Œæˆ!")
    print("=" * 60)


def parse_args():
    parser = ArgumentParser()
    parser.add_argument(
        'inputs', type=str, help='Input image file or folder path.')
    parser.add_argument(
        'model',
        type=str,
        help='Config or checkpoint .pth file or the model name')
    parser.add_argument('--weights', default=None, help='Checkpoint file')
    parser.add_argument(
        '--out-dir',
        type=str,
        default='/data/home/qr/mmdetection-main/outputimages/',
        help='Output directory of images or prediction results.')
    parser.add_argument(
        '--texts', help='text prompt, such as "bench . car .", "$: coco"')
    parser.add_argument(
        '--device', default='cuda:6', help='Device used for inference')
    parser.add_argument(
        '--pred-score-thr',
        type=float,
        default=0.3,
        help='bbox score threshold')
    parser.add_argument(
        '--batch-size', type=int, default=1, help='Inference batch size.')
    parser.add_argument(
        '--show',
        action='store_true',
        help='Display the image in a popup window.')
    parser.add_argument(
        '--no-save-vis',
        action='store_true',
        help='Do not save detection vis results')
    parser.add_argument(
        '--no-save-pred',
        action='store_true',
        help='Do not save detection json results')
    parser.add_argument(
        '--print-result',
        action='store_true',
        help='Whether to print the results.')
    parser.add_argument(
        '--palette',
        default='none',
        choices=['coco', 'voc', 'citys', 'random', 'none'],
        help='Color palette used for visualization')
    parser.add_argument(
        '--custom-entities',
        '-c',
        action='store_true',
        help='Whether to customize entity names?')
    parser.add_argument(
        '--chunked-size',
        '-s',
        type=int,
        default=-1,
        help='Chunked size for large number of categories.')
    parser.add_argument(
        '--tokens-positive',
        '-p',
        type=str,
        help='Token positions for Grounding DINO.')

    # æ–°å¢å¼‚å¸¸æ£€æµ‹å‚æ•°
    parser.add_argument(
        '--enable-anomaly-check',
        action='store_true',
        help='Enable anomaly detection for missing or incorrect detections')
    parser.add_argument(
        '--min-detections',
        type=int,
        default=1,
        help='Minimum number of detections (below this may indicate missing objects)')
    parser.add_argument(
        '--max-detections',
        type=int,
        default=100,
        help='Maximum number of detections (above this may indicate over-detection)')
    parser.add_argument(
        '--min-avg-score',
        type=float,
        default=0.4,
        help='Minimum average confidence score')

    call_args = vars(parser.parse_args())

    if call_args['no_save_vis'] and call_args['no_save_pred']:
        call_args['out_dir'] = ''

    if call_args['model'].endswith('.pth'):
        print_log('The model is a weight file, automatically '
                  'assign the model to --weights')
        call_args['weights'] = call_args['model']
        call_args['model'] = None

    if call_args['texts'] is not None:
        if call_args['texts'].startswith('$:'):
            dataset_name = call_args['texts'][3:].strip()
            class_names = get_classes(dataset_name)
            call_args['texts'] = [tuple(class_names)]

    if call_args['tokens_positive'] is not None:
        call_args['tokens_positive'] = ast.literal_eval(
            call_args['tokens_positive'])

    init_kws = ['model', 'weights', 'device', 'palette']
    init_args = {}
    for init_kw in init_kws:
        init_args[init_kw] = call_args.pop(init_kw)

    return init_args, call_args


def main():
    """
    ä¸»å‡½æ•° - ä½ å¯ä»¥é€‰æ‹©ä½¿ç”¨é…ç½®æ–‡ä»¶æ–¹å¼æˆ–å‘½ä»¤è¡Œæ–¹å¼
    """
    import sys

    # å¦‚æœæ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶æ–¹å¼
    if len(sys.argv) == 1:
        print("ä½¿ç”¨ä»£ç é…ç½®æ¨¡å¼...")
        batch_detection_with_config()
    else:
        print("ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æ¨¡å¼...")
        init_args, call_args = parse_args()

        # æå–å¼‚å¸¸æ£€æµ‹å‚æ•°
        enable_anomaly = call_args.pop('enable_anomaly_check')
        min_dets = call_args.pop('min_detections')
        max_dets = call_args.pop('max_detections')
        min_score = call_args.pop('min_avg_score')

        inferencer = DetInferencer(**init_args)

        chunked_size = call_args.pop('chunked_size')
        inferencer.model.test_cfg.chunked_size = chunked_size

        # å¦‚æœå¯ç”¨å¼‚å¸¸æ£€æµ‹ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        if enable_anomaly:
            anomaly_detector = AnomalyDetector(
                enable=True,
                min_detections=min_dets,
                max_detections=max_dets,
                min_avg_score=min_score
            )

            inputs = call_args['inputs']
            out_dir = call_args['out_dir']

            # è·å–å›¾ç‰‡åˆ—è¡¨
            if os.path.isdir(inputs):
                image_files = []
                for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
                    image_files.extend(Path(inputs).glob(f'*{ext}'))
                    image_files.extend(Path(inputs).glob(f'*{ext.upper()}'))
                image_files = sorted([str(f) for f in image_files])
            else:
                image_files = [inputs]

            # åˆ›å»ºä¸´æ—¶ç›®å½•
            temp_pred_dir = os.path.join(out_dir, 'temp_predictions') if out_dir else './temp_predictions'
            os.makedirs(temp_pred_dir, exist_ok=True)

            # é€å¼ å¤„ç†
            for img_path in image_files:
                img_name = os.path.basename(img_path)

                # ä¸´æ—¶ä¿®æ”¹è¾“å‡ºç›®å½•
                temp_call_args = call_args.copy()
                temp_call_args['inputs'] = img_path
                temp_call_args['out_dir'] = temp_pred_dir
                temp_call_args['no_save_vis'] = True
                temp_call_args['no_save_pred'] = False

                inferencer(**temp_call_args)

                # è¯»å–é¢„æµ‹ç»“æœ
                pred_json_path = os.path.join(temp_pred_dir, 'predictions.json')
                if os.path.exists(pred_json_path):
                    with open(pred_json_path, 'r') as f:
                        pred_data = json.load(f)

                    img_prediction = None
                    if isinstance(pred_data, list) and len(pred_data) > 0:
                        img_prediction = pred_data[-1]
                    elif isinstance(pred_data, dict):
                        img_prediction = pred_data

                    if img_prediction:
                        is_anomaly, reason = anomaly_detector.check_result(img_path, img_prediction)
                        if is_anomaly:
                            anomaly_detector.add_anomaly(img_path, reason)

                    os.remove(pred_json_path)

            # æ¸…ç†ä¸´æ—¶ç›®å½•
            try:
                os.rmdir(temp_pred_dir)
            except:
                pass

            # ä¿å­˜å®Œæ•´ç»“æœå’Œå¼‚å¸¸åˆ—è¡¨
            if out_dir:
                inferencer(**call_args)
                anomaly_detector.save_anomaly_list(out_dir)
        else:
            inferencer(**call_args)

        if call_args['out_dir'] != '' and not (call_args['no_save_vis']
                                               and call_args['no_save_pred']):
            print_log(f'results have been saved at {call_args["out_dir"]}')


if __name__ == '__main__':
    main()